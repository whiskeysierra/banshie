\section{Conclusion}
\label{sec:conclusion}
This chapter attempts to summarize this thesis by reviewing. It also gives an outlook for future work and research that can be done based on the results of this thesis.

\subsection{Review}
The main objective of this thesis, make an extensible, modular benchmarking framework, has been achieved. So, as a result, we have a prototype Open Source Java-based framework which runs in any \gls{OSGi}-compliant environment or embedded as a library in standalone applications.

Additionally I gave an overview over the current state of \gls{IE} , the most common \gls{IE} methods and approaches as well as an introduction to \gls{IE} evaluation methodology. We discussed different matching rules, mapping algorithms, performance and runtime performance measures for evaluating information extraction systems.

The delivered framework prototype currently supports the \textit{All Occurences} \gls{IE} approach and mixed rule approach (\enquote*{contains} and \enquote*{overlap}) for matching reference answers and predictions. It includes, but is not limited to, six different performance (precision, recall, F-Measure, Error measure, Error per response fill and slot error rate) and two runtime performance measures (CPU time and memory consumption). The current version of the software is limited to \gls{JVM}-based extractors, due to the fact that the collecting of the runtime performance measures is realized using \acs{JMX} tools.

The platform was planned to include a web based user interface which provides HTTP uploads for IE programs, monitoring execution progress as well as statistics and scores in form of diagrams. Unfortunately, due to time management issues, this feature has not been realized.

\subsection{Lessons learned}
\fxfatal{Lessons learned}
% writing modular software is not easy; an ongoing process (managing dependencies!, split modules, introduce levels of indirection to break up tightly coupled modules, ...), similar to good class/package level design, you get used to it and apply patterns without realizing you are doing s.
% evaluating extractors is not easy, no general agreement about correct answers of extraction 
% finding a output format for extractors which is easy enough to adapt to but flexible enough to handle different approaches and tasks is difficult
% writing adapters not that hard but still necessary, different APIs might be more difficult
% IO handling and XML writing in pure Java is extremely verbose, see appendix (put full adapters in appendix)

\subsection{Outlook and future work}
Banshie's architectural design is based on solid, state-of-the-art patterns, but to expand the framework's capabilities beyond prototype character several ideas come to mind. Some of these ideas will be explained and discussed on the following pages.

The current focus is clearly the field of \textit{Named Entity Recocgnition}, which is an important part of \textit{Information Extraction} and \textit{Natural Language Processing}, but there are other tasks in \gls{IE} which are equally interesting, e.g. relationship extraction, part-of-speech tagging or grammatical sentence analysis (see chapter \ref{sec:information-extraction}).

Supporting multiple \gls{IE} tasks requires Banshie to offer a more flexible XML schema. Reusing the reference- and hypothesis schema definitions proposed by GeMap comes to mind \cite{Linsmayr:2010b}.

Since Evaliex uses a different reference-hypothesis association algorithm, offering a swappable reference-hypothesis mapping algorithm for the performance evaluation could be a useful extension to the framework in order to compare results and/or to allow users to choose based on their use case.

The modified version of the \textit{General Greedy Mapping Algorithm} used in \textit{Evaliex} is based on string/word similarity. Future versions of the Banshie platform should support multiple algorithm, e.g. Levenshtein distance and Jaccard coefficient, as well as a plug-in mechanism for those similarity checks.

The current version of the framework only supports \gls{JVM}-based extractors and since different extractors have different requirements, e.g. memory and garbage collector configuration, applying additional custom command line parameters to the external Java process would be handy too.

Another stage of expansion would include alternate \texttt{Engine} implementations to support non \gls{JVM}-based extractors. Different engines would of course require different means to collect runtime events. In other words for different engines one needs to supply a viable \gls{JMX} client alternative.

The framework, in its current state, is solely an \gls{API}-based tool, which offers great embeddability for \gls{OSGi}- and likewise Java SE environments. To support more use cases providing a simple text-based \gls{CLI} seems to be a promising extension to the platform.

A Web \gls{UI}, in addition to the \gls{CLI}, would be an even more user-friendly approach. A web-based frontend could include fail-safe, responsive and intuitive interface elements to allow easy upload, querying and execution of extractors. A Web \gls{UI} would also be an excellent place to provide visual representations of statistical data and analytical results in the form of charts and diagrams.

Collecting, persisting and aggregating CPU time and memory consumption is the straigtforward approach to measure the runtime performance of a program. But other users might require different or additional measures like file system consumption, thread count or startup latency. \gls{JMX} supports many, many more indicators which could be used by alternative \textit{event production} implementations.

The memory consumption is calculated as the average heap size while the CPU time just looks at the last value. Those are just concrete implementations of generic aggregate functions: \texttt{AVG} and \texttt{LAST} in that case. A more sophisticated approach would be to support an extensible core set of aggregate functions, e.g. \texttt{MIN}, \texttt{MAX}, \texttt{AVG}, \texttt{STDDEV}, \texttt{VAR}, \texttt{SUM}, \texttt{FIRST} and \texttt{LAST}, which operate on the raw logging data. Even offering a lightweight MapReduce integration for a more flexible and user-oriented statistical analysis is imaginable.