\section{Conclusion}
\label{sec:conclusion}
This chapter summarizes the work with a review and discusses some lessons learned in the process. It also gives an outlook for future work and research that can be done based on the results of this thesis.

\subsection{Review}
The main objective of this thesis, make an extensible, modular benchmarking framework, has been achieved. So, as a result, we have a prototype Open Source Java-based framework for evaluating and benchmarking information extraction systems which runs in any \gls{OSGi}-compliant environment or embedded as a library in standalone applications.

Additionally I gave an overview over the current state of \gls{IE} , the most common \gls{IE} methods and approaches as well as an introduction to \gls{IE} evaluation methodology. We discussed different matching rules, mapping algorithms and performance and runtime performance measures for evaluating information extraction systems.

The delivered framework prototype currently supports the \textit{All Occurences} \gls{IE} approach and mixed rule approach (\enquote*{contains} and \enquote*{overlap}) for matching reference answers and predictions. It includes, but is not limited to, six different performance (precision, recall, F-Measure, Error measure, Error per response fill and slot error rate) and two runtime performance measures (CPU time and memory consumption). The current version of the software is limited to \gls{JVM}-based extractors, due to the fact that the collecting of the runtime performance measures is realized using \acs{JMX} tools.

The platform was planned to include a web based user interface which provides HTTP uploads for IE programs, monitoring execution progress as well as statistics and scores in form of diagrams. Unfortunately, due to time management issues, this feature has not been realized.

\subsection{Lessons learned}
Modularity is very common buzzword in software engineering, but as so often it's easier said than done. Writing modular software is not easy. It's an ongoing process which consists of continuous dependency management. One needs to split up and introduce levels of indirection to break up too tightly coupled modules. What even makes writing modular software more difficult is that there is no perfect way for modularity, every decission is a compromise between more coarse-grained, and easier to use, modules and more fine-grained, and easier to reuse, modules.  But on the other hand good modular architectur is very similar to good class/package level design, you get used to it and apply patterns without even realizing you are doing so.

Evaluating information extraction systems uniformly is not an easy task either. First of all, very much like the area of \gls{IE}, the process of evaluating is as inexact, because there is no general agreement about correct answers of extraction. The desired results differ from domain to domain and even from use case to use case. There might always be a human factor to defining gold standards for extraction results, since it's so highly subjective.

Another reason why it's difficult to evaluate different tools is the need to find a common data format for extractors which, on the one hand, is simple enough to be adapted to but, on the other hand, flexible enough to handle different \gls{IE} approaches and tasks. Filling templates, see \acl{OBD}, and annotating tokens in a document, see \acl{AO}, are two relatively different approaches and offering a single output format capable of supporting both approaches is not straightforward.

Writing adapters for existing \gls{IE} tools to run and test them in Banshie is not particularly hard but still necessary. The APIs of the selected extractors were reasonable simple to understand and adapt to the framework's interface specification. But different systems and their respective APIs might be more difficult.

\subsection{Outlook and future work}
Banshie's architectural design is based on solid, state-of-the-art patterns, but to expand the framework's capabilities beyond prototype character several ideas come to mind. Some of these ideas will be explained and discussed on the following pages.

The current focus is clearly the field of \textit{Named Entity Recocgnition}, which is an important part of \textit{Information Extraction} and \textit{Natural Language Processing}, but there are other tasks in \gls{IE} which are equally interesting, e.g. relationship extraction, part-of-speech tagging or grammatical sentence analysis (see chapter \ref{sec:information-extraction}).

Supporting multiple \gls{IE} tasks requires Banshie to offer a more flexible XML schema. Reusing the reference- and hypothesis schema definitions proposed by GeMap comes to mind \cite{Linsmayr:2010b}.

Since Evaliex uses a different reference-hypothesis association algorithm, offering a swappable reference-hypothesis mapping algorithm for the performance evaluation could be a useful extension to the framework in order to compare results and/or to allow users to choose based on their use case.

The modified version of the \textit{General Greedy Mapping Algorithm} used in \textit{Evaliex} is based on string/word similarity. Future versions of the Banshie platform should support multiple algorithm, e.g. Levenshtein distance and Jaccard coefficient, as well as a plug-in mechanism for those similarity checks.

The current version of the framework only supports \gls{JVM}-based extractors and since different extractors have different requirements, e.g. memory and garbage collector configuration, applying additional custom command line parameters to the external Java process would be handy too.

Another stage of expansion would include alternate \texttt{Engine} implementations to support non \gls{JVM}-based extractors. Different engines would of course require different means to collect runtime events. In other words for different engines one needs to supply a viable \gls{JMX} client alternative.

The framework, in its current state, is solely an \gls{API}-based tool, which offers great embeddability for \gls{OSGi}- and likewise Java SE environments. To support more use cases providing a simple text-based \gls{CLI} seems to be a promising extension to the platform.

A Web \gls{UI}, in addition to the \gls{CLI}, would be an even more user-friendly approach. A web-based frontend could include fail-safe, responsive and intuitive interface elements to allow easy upload, querying and execution of extractors. A Web \gls{UI} would also be an excellent place to provide visual representations of statistical data and analytical results in the form of charts and diagrams.

Collecting, persisting and aggregating CPU time and memory consumption is the straigtforward approach to measure the runtime performance of a program. But other users might require different or additional measures like file system consumption, thread count or startup latency. \gls{JMX} supports many, many more indicators which could be used by alternative \textit{event production} implementations.

The memory consumption is calculated as the average heap size while the CPU time just looks at the last value. Those are just concrete implementations of generic aggregate functions: \texttt{AVG} and \texttt{LAST} in that case. A more sophisticated approach would be to support an extensible core set of aggregate functions, e.g. \texttt{MIN}, \texttt{MAX}, \texttt{AVG}, \texttt{STDDEV}, \texttt{VAR}, \texttt{SUM}, \texttt{FIRST} and \texttt{LAST}, which operate on the raw logging data. Even offering a lightweight MapReduce integration for a more flexible and user-oriented statistical analysis is imaginable.