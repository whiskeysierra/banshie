\section{Information Extraction}
\label{sec:information-extraction}

\fxfatal{Information Extraction}
\fxfatal{Subfield of NLP}

\subsection{Definition}

\begin{quote}
Information Extraction is a technology that is futuristic from the user's point of view in the current information-driven world. Rather than indicating which documents need to be read by a user, it extracts pieces of information that are salient to the user's needs. Links between the extracted information and the original documents are maintained to allow the user to reference context. The kinds of information that systems extract vary in detail and reliability.

\hfill \textbf{Message Understanding Conference (MUC)}

\hfill \citeauthor{Chinchor:2001} \cite{Chinchor:2001}
\end{quote}

\begin{quote}
Information Extraction refers to the automatic extraction of structured information such as entities, relationships between entities, and attributes describing entities from unstructured sources. This enables much richer forms of queries on the abundant unstructured sources than possible with keyword searches alone. When structured and unstructured data co-exist, information extraction makes it possible to integrate the two types of sources and pose queries spanning them.

\hfill \textbf{Information Extraction}

\hfill \citeauthor{Sarawagi:2008} \cite{Sarawagi:2008}
\end{quote}

\begin{quote}
Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.

\hfill \textbf{Information extraction}

\hfill \citeauthor{Wikipedia:IE:2012} \cite{Wikipedia:IE:2012}
\end{quote}

\newpage
\subsection{History}
The area of text understanding can be considered as the basis for the \gls{IE}. In this regard, researchers studied methods in the field of Artificial Intelligence, which reproduce the contents of a text in exact form \cite{Siefkes:2007}\cite{Eikvil:1999}. The first application of information extraction occurred in the 1950s, were  the information from texts  were reduced into a table structure. Sager 1981 published works in the field of medicine and used manually-created structures and templates. This "information formats" were obtained based on rules. Complex system developments were made by Hayes et al. in 1992 \cite{Grishman:1997}\cite{Gaizauskas:1998}\cite{Wilks:1997}.

The increase in research in the field of IE forced the \gls{DARPA} in the late 1980s to initiate an operation. Thus, the \gls{MUC} have been launched, aimed at competing implementation and evaluation of IE systems. Participants received test data of a particular domain and a special output format. They than developed IE systems based on these requirements, their performances were compared in terms of unknown documents at conferences. Manually created templates were used as reference data \cite{Grishman:1996}\cite{Grishman:1997}.

The conferences were held between 1987 and 1998. The following table lists the domains and the number of training and reference documents of the respective conferences \cite{Turmo:2006}\cite{Appelt:1999}\cite{Cunningham:2005}:

\begin{table}[H]
\centering
\begin{tabular*}{\textwidth}{ l l l l l l }
	\toprule
	& Year & Topic & \shortstack{Number of \\ systems} & \shortstack{Traning \\ documents} & \shortstack{Reference \\ documents} \\
	\midrule
	MUC-1 & 1987 & Marine operations & 6 & 12 & 2 \\
	MUC-2 & 1989 & Marine operations & 8 & 105 & 25 \\
	MUC-3 & 1991 & Terror acts & 15 & 1300 & 300 \\
	MUC-4 & 1992 & Terror acts & 17 & - & - \\
	MUC-5 & 1993 & Joint ventures, & 17 & - & - \\
	& & microelectronics & & & \\
	MUC-6 & 1995 & Management & 17 & - & - \\
	& & changes & & & \\
	MUC-7 & 1998 & Space travel & - & - & - \\
	\bottomrule
\end{tabular*}
\caption{Message Understanding Conferences}
\end{table}

The conferences have made a decisive contribution to the development of information extraction. On one hand, the formulation of sub-tasks and metrics should be noted and on the other hand, the striving for domain independence and portability of \gls{IE} systems. The meeting of various research groups and the implementation of systems based on the same task offers enormous opportunities to exchange ideas and to overcome theoretical and paradigmatic differences \cite{Cimiano:2003}\cite{Lehnert:1994}.

\newpage
\subsection{Most typical tasks}

The Message Understanding Conferences structures the information extraction into the following sub-tasks due to its complexity \cite{Carstensen:2010}\cite{Lavelli:2008}:

The \gls{IE} sub-tasks will be explained using the following example document:

\begin{quote}
The shiny red rocket was fired on Tuesday. It is the brainchild of Dr. Big Head. Dr. Head is a staff scientist at We Build Rockets Inc.
\cite{Cunningham:2005}
\end{quote}

\subsubsection{Named Entity Recognition}
\gls{NER}, also referred to as Name Recognition, Entity Identification or Entity Extraction, is defined as the extraction of known entity names. These include people, organizations, locations, products, date/times and certain numerical expressions \cite{Linsmayr:2010}. 

\begin{table}[H]
\centering
\begin{tabular*}{\textwidth}{ l  l }
	\toprule
	\textbf{Type} & \textbf{Value} \\
	\midrule
	\texttt{PRODUCT} & rocket \\
	\texttt{DATE} & Tuesday \\
	\texttt{PERSON} & Dr. Big Head \\
	\texttt{ORGANIZATION} & We Build Rockets Inc. \\
	\bottomrule
\end{tabular*}
\caption{Named Entity Recognition example output}
\end{table}

\subsubsection{Coreference Resolution}
\gls{CO}, also referred to as Coreference Analysis, Deduplication or Record Linkage. As entities and relationships are extracted from the unstructured source, they need to be integrated with existing databases and with repeated mentions of the same information in the unstructured source. The main challenge in this task is deciding if two strings refer to the same entity in spite of the many noisy variants in which it appears in the unstructured source \cite{Sarawagi:2008}.

Example: \textit{It} in \enquote{It is the brainchild of Dr. Big Head. Dr. Head is a staff scientist at We Build Rockets Inc.} refers to the previously extracted entity \textit{rocket}.

\subsubsection{Template Element Construction}
\gls{TE}, also referred to as Attribute Extraction, describes the task to associate a given entity with the value of an adjective describing the entity. The value of this adjective typically needs to be derived by combining soft clues spread over many different words around the entity
 \cite{Sarawagi:2008}.

\begin{table}[H]
\centering
\begin{tabular*}{\textwidth}{ l  l }
	\toprule
	\textbf{Attribute} & \textbf{Target} \\
	\midrule
	shiny red & rocket \\
	brainchild of Dr. Big Head & rocket \\
	\bottomrule
\end{tabular*}
\caption{Template Element Construction example output}
\end{table}

\subsubsection{Template Relation Construction}
\gls{TR}, also referred to as Relationship extraction, defines to task of extracting relationship information of previously extracted entities. Relationships are defined over two or more entities related in a predefined way. Examples are \enquote{is employee of} relationship between a person and an organization or \enquote{is acquired by} relationship between pairs of companies \cite{Sarawagi:2008}.

The extraction of relationships differs from the extraction of entities in one significant way. Whereas entities refer to a sequence of words in the source and can be expressed as annotations on the source, relationships are not annotations on a subset of words. Instead they express the associations between two separate text snippets representing the entities \cite{Sarawagi:2008}.

\begin{table}[H]
\centering
\begin{tabular*}{\textwidth}{ l l l l l }
	\toprule
	\textbf{Entity} & \textbf{Relation}  & \textbf{Entity} \\
	\midrule
	Dr. Big Head & works for & We Build Rockets Inc. \\
	\bottomrule
\end{tabular*}
\caption{Template Element Construction example output}
\end{table}

\newpage
\subsubsection{Scenario Template Production}
\gls{ST}, also referred to as Event Extraction, tries to extract events that previously extracted entities participate in \cite{Cunningham:2005}.

Regarding the given example document, \gls{ST} discovers that there was a rocket launching event in which the various entities were involved \cite{Cunningham:2005}.

\subsubsection{Restoring information structure such as Lists, Tables and Ontologies}
The scope of extraction systems has now expanded to include the extraction of not such atomic entities and flat records but also richer structures such as tables, lists, and trees from various types of documents \cite{Sarawagi:2008}.

\newpage
\subsection{Development and progress}
This chapter describes different approaches for the construction of an IE system as well as the current research in the field of information extraction.

There are different approaches for the construction of an IE system which are divided into methods of knowledge engineering and machine learning. It should be noted that an exact categorization is usually not possible because many procedures are a combination of both approaches \cite{Schramm:2008}.

The current research in the field of information extraction relates to the extraction of HTML pages, the portions of the IE and the automatic addition of annotations with the aid of ontologies \cite{Linsmayr:2010}.

\subsubsection{Knowledge Engineering}
The method of Knowledge Engineering is the manual creation of grammar by a human expert. The identification can be done either by comparing them to a list or by applying rules. Domain knowledge, which is not always available, is necessary to specify extraction rules. Experts must find patterns by inspecting the corpus and produce guidelines according to these patterns \cite{Schramm:2008}\cite{Turmo:2006}.

The process is usually implemented iteratively. First, the definition of grammar rules, which are tested on a training corpus. The rules may be modified depending on the results. The steps are repeated to achieve an acceptable output \cite{Appelt:1999}.

\subsubsection{Machine Learning}
This approach focuses on the extraction based on specific learning process. Can be made between these methods to the degree of supervision \cite{Carstensen:2010}\cite{Schramm:2008}:

\paragraph{Supervised learning}
This is based on a manually annotated corpus, which contains postitive and negative examples of entities. This static feature combinations are used for the extraction of entities and relations. Here, the probability is calculated that it is the extracted data is the desired entities. This range includes learning methods like \gls{HMM} and \gls{SVM} \cite{Schramm:2008}\cite{Siefkes:2005}\cite{Carstensen:2010}.

\paragraph{Semi-supervised learning}
In this method, a corpus with a small amount of annotations (seeds) is supplied to. During the application phase the seeds with the best combination of features for customizing existing rules and creating new ones are located and used. This approach is referred to as bootstrapping \cite{Carstensen:2010}\cite{Chang:2006}.

\paragraph{Unsupervised learning}
The method of unsupervised learning requires no annotations and manually generated training data. The system will only be given a set of entities whose properties are analyzed. The knowledge gained is the basis for the localization of entities \cite{Carstensen:2010}\cite{Schramm:2008}.

\subsubsection{Wrapper Generation}
The rise of the textual sources on the internet brings an adaptation of existing approaches to extraction. HTML pages are different from text documents, because they contain so-called formatting tags and descriptions. These can, in addition to the page content, contain relevant data. Furthermore, HTML documents contain links to other sites, which may also have relevant knowledge. Because of these challenges investigations regarding wrappers were launched  \cite{Eikvil:1999}\cite{Freitag:2000}\cite{Linsmayr:2010}.

A wrapper is a procedure that identifies data from a source in accordance with special extraction rules. The information within HTML pages are converted into a format explicitly stored for further use. A wrapper must coincide with the dynamic content of the web, manage the change of links and formatting errors. Since a wrapper is limited to a source, research in the area of \gls{WG} is initiated \cite{Chang:2006}\cite{Eikvil:1999}.

The wrapper generation focuses on the integration of multiple sources and counteracts the heterogeneity of the web by using a wrapper library \cite{Siefkes:2007}\cite{Turmo:2006}.

\subsubsection{Automatic Content Extraction}
The program of the \gls{ACE} (\url{http://www.itl.nist.gov/iad/mig/tests/ace/})  was launched as a successor to the \gls{MUC} in 1999. It's the  research for automatic processing of natural language texts and the development of necessary systems. The field of \gls{IE} is divided into the following sections \cite{Nist:2008}\cite{Lavelli:2008}\cite{Turmo:2006}\cite{Linsmayr:2010}:

\begin{itemize}
\item \textbf{\gls{EDR}} \newline
Identification of entity types and subcategories
\item \textbf{\gls{RDR}} \newline
Recognition of relationships between entities
\item \textbf{\gls{VDR}} \newline
Extraction of events and scenarios in which entities are involved
\end{itemize}

Unlike the \gls{MUC} evaluation results of the program will not be published. Also the tasks of the \gls{ACE} are more complex, as multiple domains are used and also multiple sources of language translation, or the \gls{OCR} need to be analyzed \cite{Cunningham:2005}.

\subsubsection{Ontology-based Information Extraction}
The idea of the Semantic Web is an extension of traditional content so-called annotations. To realize this requires the creation of annotations, the linking of websites with ontologies and the establishment and management of ontologies. Under an ontology a knowledge model is understood that represents concepts and terms, and their relationships. In this context, studies have been started in the field of \gls{OBIE} which serves the automation of these processes. Unlike traditional information extraction not only the extraction of an entity is focused, but also the image on an ontology \cite{Cunningham:2005}\cite{Maynard:2005}\cite{Weinhofer:2010}\cite{Linsmayr:2010}.

\gls{OBIE} faces the following challenges:

\newpage
\subsection{Related fields}

\subsubsection{Information Retrieval}

\subsubsection{Natural Language Processing}
\gls{NLP} ...

\subsubsection{Machine Learning}
