\section{Results}
\label{sec:results}
This chapter aims to demonstrate how existing information extraction systems can be evaluated with Banshie. For evaluation purposes two freely available \gls{NER} taggers were chosen. A test document, reference output and examplary adapter implementations for the selected systems will be shown and discussed over the course of the following pages.

\subsection{Open Source Extractors}
Several \gls{NLP} tools and suites exist but for the purpose of evaluating Banshie only two were chosen. The choice came down to Apache OpenNLP and Stanford CoreNLP because both are well-known tools from respected authorities in Open Source Software and Natural Language Processing: in this case the \acs{ASF} and Christopher Manning\footnote{\url{http://nlp.stanford.edu/~manning/}} respectively. A second reason for selecting those two is that both are written in Java and available via the Maven Central repository.

\subsubsection{Apache OpenNLP}
Apache OpenNLP\footnote{\url{http://opennlp.apache.org/}} is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. \cite{OpenNLP}

The Apache OpenNLP project is written in Java, licensed under the Apache License Version 2.0, available on Maven Central and completed incubation on February 2012 and is now an Apache top level project.

\subsubsection{Stanford CoreNLP}
Stanford CoreNLP
\footnote{\url{http://nlp.stanford.edu/software/corenlp.shtml}} is an integrated framework, which make it very easy to apply a bunch of language analysis tools to a piece of text. Stanford CoreNLP integrates tools, like the a part-of-speech tagger, a named entity recognizer, a parser, and a coreference resolution system, and provides model files for analysis of English. The goal of the project is to enable people to quickly and painlessly get complete linguistic annotations of natural language texts. It is designed to be highly flexible and extensible. \cite{CoreNLP}

The Stanford CoreNLP code is written in Java, licensed under the GNU General Public License and available on Maven Central.

\newpage
\subsection{Examples}
\fxfatal{Introduction}
For the sake of readability of this document and to keep the evaluation process understandable an extremely small test document, shown in listing \ref{lst:example-document}, was chosen.

\begin{listing}[H]
\texttt{\input{example.txt}}
\caption{Example document}
\label{lst:example-document}
\end{listing}

The example reference output, listing \ref{lst:example-reference}, was created manually by annotating the example document with xml tags. It should be noted, that an editor visualizing the UTF-8 character offset is extremely helpful when creating a reference output by hand. Another possibility is to take the output of one extractor and modify the result accordingly.

\begin{listing}[H]
\inputminted{xml}{reference.xml}
\caption{Example extraction reference}
\label{lst:example-reference}
\end{listing}

\newpage
\subsubsection{Apache OpenNLP}
Listing \ref{lst:opennlp-adapter} shows the most relevant part of the adapter implementation for the Apache OpenNLP Name Finder Tool, with omitting the document reading and output writing part of the code. The three most important services provided by OpenNLP are the \texttt{SentenceDetector}, the \texttt{Tokenizer} and the \texttt{NameFinder}. The SentenceDetector and Tokenizer are used to split up the input document into sentences and sentences into tokens respectively. The resulting lists of tokens are than passed on to the NameFinder which finds spans and marks them as persons, organizations or locations accordingly.

\begin{listing}[H]
\begin{minted}{java}
final SentenceDetector detector = getSentenceDetector();
final Tokenizer tokenizer = getTokenizer();
final TokenNameFinder finder = getNameFinder();

for (Span sentences : detector.sentPosDetect(document)) {
    final String sentence = sentences.getCoveredText(document);
    final Span[] indices = tokenizer.tokenizePos(sentence);
    final String[] tokens = spansToStrings(indices, sentence);
    final Span[] spans = finder.find(tokens);
    final List<String> words = Arrays.asList(tokens);

    for (Span span : spans) {
        final String word = joiner.join(
            words.subList(span.getStart(), span.getEnd()));
        final String type = span.getType();
        final int start = sentences.getStart() + 
            indices[span.getStart()].getStart();
        final int end = sentences.getStart() + 
            indices[span.getEnd()].getEnd() - 1;

        // ...
    }
}
\end{minted}
\caption{Apache OpenNLP extractor adapter}
\label{lst:opennlp-adapter}
\end{listing}

\newpage
Listing \ref{lst:opennlp-result} shows the extraction result produced by the adapter implementation.

\begin{listing}[H]
\inputminted{xml}{opennlp.xml}
\caption{Apache OpenNLP extraction result}
\label{lst:opennlp-result}
\end{listing}

The evaluation result of the extraction outcome and the recorded event log produced by the default \texttt{PerformanceEvaluator} and \texttt{QualityEvaluator} implementations, discussed in chapter \ref{sec:design}, is shown in following listing.

\begin{listing}[H]
\inputminted{java}{opennlp.txt}
\caption{Apache OpenNLP evaluation result}
\label{lst:opennlp-eval-result}
\end{listing}

As shown in listing \ref{lst:opennlp-eval-result}, OpenNLP only required about 2.25 seconds CPU time and 35 megabytes of memory to perform the extraction but on the other it only scored a recall of .3 and a F-measure of .5, which means the OpenNLP tagger only found a third of what could have been found, according to our reference output.

\newpage
\subsubsection{Stanford CoreNLP}
\fxfatal{Stanford CoreNLP}

\begin{listing}[H]
\begin{minted}{java}
final Annotation annotation = new Annotation(document);

final Properties properties = new Properties();
properties.put("annotators", "tokenize, ssplit, pos, lemma, ner");
final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);

pipeline.annotate(annotation);

final List<CoreMap> sentences = 
    annotation.get(SentencesAnnotation.class);

for (CoreMap sentence : sentences) {
    for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
        final String word = 
            token.get(TextAnnotation.class);
        final String type = 
            token.get(NamedEntityTagAnnotation.class);
        final int start = token.beginPosition();
        final int end = token.endPosition();

        // ...
    }
}
\end{minted}
\caption{Stanford CoreNLP extractor adapter}
\label{lst:corenlp-adapter}
\end{listing}

\begin{listing}[H]
\inputminted{xml}{corenlp.xml}
\caption{Stanford CoreNLP extraction result}
\label{lst:corenlp-result}
\end{listing}

\begin{listing}[H]
\inputminted{java}{corenlp.txt}
\caption{Stanford CoreNLP evaluation result}
\end{listing}

\newpage
\subsection{Summary}
\fxfatal{Outro}
%complexity, effort, experiences, ...
% roughly 100 loc each