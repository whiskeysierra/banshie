\section{Results}
\label{sec:results}
This chapter aims to show the results of the framework implementation by demonstrating how existing information extraction systems can be evaluated with Banshie. For evaluation purposes two freely available \gls{NER} taggers were chosen. A test document, reference output and examplary adapter implementations for the selected systems will be shown and discussed over the course of the following pages.

\subsection{Open Source Extractors}
Several \gls{NLP} tools and suites exist but for the purpose of evaluating Banshie only two were chosen. The choice came down to Apache OpenNLP and Stanford CoreNLP because both are well-known tools from respected authorities in Open Source Software and Natural Language Processing: in this case the \acs{ASF} and Christopher Manning\footnote{\url{http://nlp.stanford.edu/~manning/}} respectively. A second reason for selecting those two is that both are written in Java and available via the Maven Central repository.

\subsubsection{Apache OpenNLP}
Apache OpenNLP\footnote{\url{http://opennlp.apache.org/}} is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. \cite{OpenNLP}

The Apache OpenNLP project is written in Java, licensed under the Apache License Version 2.0, available on Maven Central and completed incubation on February 2012 and is now an Apache top level project.

\subsubsection{Stanford CoreNLP}
Stanford CoreNLP
\footnote{\url{http://nlp.stanford.edu/software/corenlp.shtml}} is an integrated framework, which make it very easy to apply a bunch of language analysis tools to a piece of text. Stanford CoreNLP integrates tools, like the a part-of-speech tagger, a named entity recognizer, a parser, and a coreference resolution system, and provides model files for analysis of English. The goal of the project is to enable people to quickly and painlessly get complete linguistic annotations of natural language texts. It is designed to be highly flexible and extensible. \cite{CoreNLP}

The Stanford CoreNLP code is written in Java, licensed under the GNU General Public License and available on Maven Central.

\newpage
\subsection{Examples}
This chapter will show the most relevant parts of the adapter implementations to make the selected \gls{NER} tools compatible to the extractor interface specification described in chapter \ref{sec:extractor-specification}. For the sake of readability of this document and to keep the evaluation process understandable an extremely small test document, shown in listing \ref{lst:example-document}, was chosen.

\begin{listing}[H]
\texttt{\input{example.txt}}
\caption{Example document}
\label{lst:example-document}
\end{listing}

The example reference output, listing \ref{lst:example-reference}, was created manually by annotating the example document with xml tags. It should be noted, that an editor visualizing the UTF-8 character offset is extremely helpful when creating a reference output by hand. Another possibility is to take the output of one extractor and modify the result accordingly.

\begin{listing}[H]
\inputminted{xml}{reference.xml}
\caption{Example extraction reference}
\label{lst:example-reference}
\end{listing}

\newpage
\subsubsection{Apache OpenNLP}
Listing \ref{lst:opennlp-adapter} shows the most relevant part of the adapter implementation for the Apache OpenNLP Name Finder Tool, with omitting the document reading and output writing part of the code. The three most important services provided by OpenNLP are the \texttt{SentenceDetector}, the \texttt{Tokenizer} and the \texttt{NameFinder}. The SentenceDetector and Tokenizer are used to split up the input document into sentences and sentences into tokens respectively. The resulting lists of tokens are than passed on to the NameFinder which finds spans and marks them as persons, organizations or locations accordingly.

\begin{listing}[H]
\begin{minted}{java}
final SentenceDetector detector = getSentenceDetector();
final Tokenizer tokenizer = getTokenizer();
final TokenNameFinder finder = getNameFinder();

for (Span sentences : detector.sentPosDetect(document)) {
    final String sentence = sentences.getCoveredText(document);
    final Span[] indices = tokenizer.tokenizePos(sentence);
    final String[] tokens = spansToStrings(indices, sentence);
    final Span[] spans = finder.find(tokens);
    final List<String> words = Arrays.asList(tokens);

    for (Span span : spans) {
        final String word = joiner.join(
            words.subList(span.getStart(), span.getEnd()));
        final String type = span.getType();
        final int start = sentences.getStart() + 
            indices[span.getStart()].getStart();
        final int end = sentences.getStart() + 
            indices[span.getEnd()].getEnd() - 1;

        // ...
    }
}
\end{minted}
\caption{Apache OpenNLP extractor adapter}
\label{lst:opennlp-adapter}
\end{listing}

OpenNLP's \gls{NER} is primarily designed to work with sentences, but since our schema requires character offsets, some additional index counting is requiring.

\newpage
Listing \ref{lst:opennlp-result} shows the extraction result produced by the adapter implementation.

\begin{listing}[H]
\inputminted{xml}{opennlp.xml}
\caption{Apache OpenNLP extraction result}
\label{lst:opennlp-result}
\end{listing}

The evaluation result of the extraction outcome and the recorded event log produced by the default \texttt{PerformanceEvaluator} and \texttt{QualityEvaluator} implementations, discussed in chapter \ref{sec:design}, is shown in following listing.

\begin{listing}[H]
\inputminted{java}{opennlp.txt}
\caption{Apache OpenNLP evaluation result}
\label{lst:opennlp-eval-result}
\end{listing}

The discrepancy between CPU time and execution is based on the fact that the process CPU time provided by \texttt{OperatingSystemMXBean} is of nanoseconds precision but not necessarily nanoseconds accuracy\cite{OperatingSystemMXBean} while the system time is in milliseconds. During extremely short startup times, like in this case, it might cause the values to inaccurate.

As shown in listing \ref{lst:opennlp-eval-result}, OpenNLP only required about 2.25 seconds CPU time and 35 megabytes of memory to perform the extraction but on the other it only scored a recall of .3 and a F-measure of .5, which means the OpenNLP tagger only found a third of what could have been found, according to our reference output.

\subsubsection{Stanford CoreNLP}
The \gls{API} provided by Stanford CoreNLP is different from Apache OpenNLP as it is designed a pipelines. It's possible to add different annotators to a pipeline which work on any document passed down the pipeline. The most common annotators are \texttt{tokenize} (tokenizing), \texttt{ssplit} (sentence splitting), \texttt{pos} (part-of-speech tagging) and \texttt{ner} (named entity recognition). The results produced by the annotators can then be retrieved by using the corresponding \texttt{get}-methods on sentence or token instances as shown in listing \ref{lst:corenlp-adapter}.

\begin{listing}[H]
\begin{minted}{java}
final Annotation annotation = new Annotation(document);

final Properties properties = new Properties();
properties.put("annotators", "tokenize, ssplit, pos, lemma, ner");
final StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);

pipeline.annotate(annotation);

final List<CoreMap> sentences = 
    annotation.get(SentencesAnnotation.class);

for (CoreMap sentence : sentences) {
    for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
        final String word = 
            token.get(TextAnnotation.class);
        final String type = 
            token.get(NamedEntityTagAnnotation.class);
        final int start = token.beginPosition();
        final int end = token.endPosition();

        // ...
    }
}
\end{minted}
\caption{Stanford CoreNLP extractor adapter}
\label{lst:corenlp-adapter}
\end{listing}

\newpage
The extraction of the CoreNLP adapter is shown in listing \ref{lst:corenlp-result} and it's obvious that it contains much more annotated tokens than the OpenNLP result in listing \ref{lst:opennlp-result}. But since the Stanford \gls{NER} tagger works on tokens, it tags individual words with \gls{NER} types.

\begin{listing}[H]
\inputminted{xml}{corenlp.xml}
\caption{Stanford CoreNLP extraction result}
\label{lst:corenlp-result}
\end{listing}

\newpage
Listing \ref{lst:corenlp-eval-result} shows the evaluation result of the CoreNLP adapter.

\begin{listing}[H]
\inputminted{java}{corenlp.txt}
\caption{Stanford CoreNLP evaluation result}
\label{lst:corenlp-eval-result}
\end{listing}

It quickly becomes apparent, that Stanford CoreNLP produces perfect extraction results, according to our example reference, but does so on the expense of required CPU time and memory consumption. 

\begin{table}[H]
\centering
\begin{tabular*}{\textwidth}{rccccccccc}
	\toprule
	Name & $t_{cpu}$ & $m$ & $t$ & $\pi$ & $\rho$ & $F$ & $E$ & $ERR$ & $SE$ \\
	\midrule
	OpenNLP & 2250.0 & 35.0 & 2001.0 & 1.0 & 0.33 & 0.5 & 0.5 & 0.66 & 0.66 \\ 
	CoreNLP & 40990.0 & 575.0 & 44943.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 0.0 \\
	\bottomrule
\end{tabular*}
\caption{Evaluation result comparison}
\label{tbl:result-comparison}
\end{table}

Table \ref{tbl:result-comparison} shows the evaluation results of both extractors in directo comparison.

CoreNLP actually spends a lot of time loading training models, but since OpenNLP also loads model data during startup, comparing the total execution time is still fair. But it should be noted, that on consecutive run, CoreNLP should perform much better. One could even imagine to use different models to trade startup time for extraction quality. Which is exactly the kind of comparison Banshie tries to offer to its clients.

\newpage
\subsection{Summary}
Writing adapters for extractors to allow execution and evaluation in the Banshie framework requires some manual work, in case of the example integrations about 100 lines of codes each. Adapting an \gls{API} to the extractor interface specification also differs greatly from extractor to extractor and highly depends on how easy it is to tokenize the input document, run the recognition process, iterate over annotated tokens and produce the desired \gls{XML} output. Especially calculating the UTF-8 character offsets can be errorprone, as listing \ref{lst:opennlp-adapter} illustrated.

Even though the example document and reference output were extremely small, the result is a nice example of a evaluation result one would expect to see a lot: Two different extractors with highly different quality-performance tradeoffs. OpenNLP only used 35 megabytes of memory and finished after about 2 seconds but only scored an F-measure of .5. CoreNLP on the other hand performed a perfect extraction but required more than 16 times the memory and 18 times the CPU time compared to OpenNLP.

Comparing and evaluating those results now highly depends on the use case, a fast and mediocre reliable extractor might be the perfect tool for one task, while another might require the best extraction, no matter the resource cost.