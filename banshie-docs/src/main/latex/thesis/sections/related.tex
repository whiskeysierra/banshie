\section{Related Work}
\label{sec:related-work}

\subsection{Evaliex}
\textit{Evaliex} is an \gls{IE} evaluation tool which integrates these measurement concepts, like state-of-the-art scoring metrics, measuring string and semantic similarities and by parameterization of metric scoring, and provides an efficient user interface that supports evaluation control and the visualization of \gls{IE} results. To guarantee domain independence, the tool additionally provides a Generic Mapper for XML Instances (GeMap) that maps domain-dependent XML files containing \gls{IE} results to generic ones. Compared to other tools, it provides more flexible testing and better visualization of extraction results for the comparison of different (versions of) information extraction systems \cite{Feilmayr:2012}.

\textit{Evaliex}  was part of a master thesis by \citeauthor{Linsmayr:2010} in 2010. A corresponding paper was published by \citeauthor{Feilmayr:2012} later in 2012:

\begin{quote}
\fullcite{Linsmayr:2010}
\end{quote}

\begin{quote}
\fullcite{Feilmayr:2012}
\end{quote}

\subsection{GATE}
The \gls{GATE} \footnote{\url{http://gate.ac.uk/}} is a free open-source infrastructure for developing and deploying software components that process human language. It is more than 15 years old and is in active use for all types of computational tasks involving language (frequently called natural language processing, text analytics, or text mining). \gls{GATE} excels at text analysis of all shapes and sizes. From large corporations to small startups, from multi-million research consortia to undergraduate projects, our user community is the largest and most diverse of any system of this type, and is active world-wide. This book contains a highly accessible introduction to \gls{GATE} Version 6 and is the first port of call for all \gls{GATE}-related questions. It includes a guide to using \gls{GATE} Developer and \gls{GATE} Embedded, and chapters on all major areas of functionality, such as processing multiple languages and large collections of unstructured text \cite{Cunningham:2011}.

The evaluation in \gls{GATE} is provided by a component called the \textit{AnnotationDiff Tool} which compares the individual annotations of a hypothesis with a reference. The differences are listed and visualized in color. \gls{GATE} calculates the metrics recall, precision and F-measure \cite{Linsmayr:2010}.

\subsection{Ellogon}
Ellogon\footnote{\url{http://www.ellogon.org/}} is a multi-lingual, cross-platform, general-purpose language engineering environment, developed in order to aid both researchers who are doing research in computational linguistics, as well as companies who produce and deliver language engineering systems. Ellogon as a language engineering platform offers an extensive set of facilities, including tools for processing and visualising textual/HTML/XML data and associated linguistic information, support for lexical resources (like creating and embedding lexicons), tools for creating annotated corpora, accessing databases, comparing annotated data, or transforming linguistic information into vectors for use with various machine learning algorithms \cite{Ellogon}.

The deviation calculation of two collections of documents is provided by the \textit{Collection Comparison Tool}. It compares the annotations and attributes. After association it calculates precision, recall and F-measure \cite{Linsmayr:2010}.

\subsection{ANNALIST}
\gls{ANNALIST} is a scoring system for the evaluation of the output of semantic annotation systems. \gls{ANNALIST} has been designed as a system that is easily extensible and configurable for different domains, data formats, and evaluation tasks. The system architecture enables data input via the use of plugins and the users can access the systemâ€™s internal alignment and scoring mechanisms without the need to convert their data to a specified format. Although developed for evaluation tasks that involve the scoring of entity mentions and relations primarily, \gls{ANNALIST}'s generic object representation and the availability of a range of criteria for the comparison of annotations enable the system to be tailored to a variety of scoring jobs \cite{Demetriou:2008}.

\gls{ANNALIST} is, in contrast to the previously described systems, a pure evaluation tool. The data can be imported via special plug-ins and is processed by individual modules. The \textit{Alignment Tool} associates hypotheses and references for each annotation type. The subsequent metric calculation is performed by the scoring module which determines precision, recall and the F-measure. The output module visualizes the results in a table \cite{Linsmayr:2010}.