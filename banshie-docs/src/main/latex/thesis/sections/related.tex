\section{Related Work}
\label{sec:related-work}
This chapter aims to provide a quick overview of some of the better known frameworks for evaluating information extraction systems.

\subsection{Evaliex}
\textit{Evaliex} is an \gls{IE} evaluation tool which integrates measurement concepts, like state-of-the-art scoring metrics, measuring string and semantic similarities and by parameterization of metric scoring, and provides an efficient user interface that supports evaluation control and the visualization of \gls{IE} results.

To guarantee domain independence, the tool additionally provides a Generic Mapper for XML Instances (GeMap) which maps domain-dependent XML files containing \gls{IE} results to generic ones. Compared to other tools, it provides more flexible testing and better visualization of extraction results for the comparison of different (versions of) information extraction systems \cite{Feilmayr:2012}.

\textit{Evaliex}  was part of a master thesis by \citeauthor{Linsmayr:2010} in 2010: \citetitle{Linsmayr:2010} \cite{Linsmayr:2010}. A corresponding paper was published by \citeauthor{Feilmayr:2012} later in 2012: \citetitle{Feilmayr:2012} \cite{Feilmayr:2012}.

Although Evaliex is a promising tool with a rich feature set; it's not available for other parties, as neither the tool itself nor the source code has yet been published. Evaliex also lacks a proper \gls{API} as it is purely designed to be a standalone desktop application providing a Java-based user interface.

\subsection{GATE}
The \gls{GATE}\footnote{\url{http://gate.ac.uk/}} is a Free \& Open-Source infrastructure for developing and deploying software components that processes human language. It is more than 15 years old and in active use for all types of computational tasks involving language. GATE excels at text analysis of all shapes and sizes. \cite{Cunningham:2011}.

The evaluation in GATE is provided by a component called the \textit{AnnotationDiff Tool} which compares the individual annotations of a hypothesis with a reference. The differences are listed and visualized in color. GATE calculates the metrics recall, precision and F-measure \cite{Linsmayr:2010}.

Similar to Evaliex the GATE Annotation Diff Tool only supports a graphical user interface and is not designed to be embedded in or used by other systems. The tool also lacks error measures, by name error measure, error per response fill and slot error rate.

\subsection{Ellogon}
Ellogon\footnote{\url{http://www.ellogon.org/}} is a multi-lingual, cross-platform, general-purpose language engineering environment, developed in order to aid both researchers who are doing research in computational linguistics, as well as companies who produce and deliver language engineering systems. Ellogon, as a language engineering platform, offers an extensive set of facilities, including tools for processing and visualizing textual/HTML/XML data and associated linguistic information, support for lexical resources, tools for creating annotated corpora, accessing databases, comparing annotated data, or transforming linguistic information into vectors for use with various machine learning algorithms \cite{Ellogon}.

The deviation calculation of two collections of documents is provided by the \textit{Collection Comparison tool}. It compares the annotations and attributes. After association it calculates precision, recall and F-measure \cite{Linsmayr:2010}.

The Ellogon Collection Comparison tool ships, very much like the Gate Annotation Diff Tool, only with a graphical user interface and lacks an API and the possibility to be embedded in other tools as a library. Ellogon also only supports the performance figures precision, recall and F-measure.

\subsection{ANNALIST}
\gls{ANNALIST}\footnote{\url{http://annalist.sourceforge.net/}} is a scoring system for the evaluation of the output of semantic annotation systems. ANNALIST has been designed as a system that is easily extensible and configurable for different domains, data formats, and evaluation tasks. The system architecture enables data input via the use of plugins and the users can access the systemâ€™s internal alignment and scoring mechanisms without the need to convert their data to a specified format. Although primarily developed for evaluation tasks that involve the scoring of entity mentions and relations, ANNALIST's generic object representation and the availability of a range of criteria for the comparison of annotations enables the system to be tailored to a variety of scoring jobs \cite{Demetriou:2008}.

ANNALIST is, in contrast to the previously described systems, a pure evaluation tool. The data can be imported via special plug-ins and is processed by individual modules. The \textit{Alignment Tool} associates hypotheses and references for each annotation type. The subsequent metric calculation is performed by the scoring module which determines precision, recall and the F-measure. The output module visualizes the results in a table \cite{Linsmayr:2010}.

ANNALIST meets the requirement of this thesis to offer an extensible evaluation tool by providing a plug-in mechanism. Plug-ins enable the tool to accept other input formats. In other words plug-ins translate between different input formats and required formats for the scoring module. ANNALIST comes with a \gls{CLI} and is therefore not developed with embeddability in mind.

\subsection{Summary}
All of these systems offer information extraction evaluation, some more sophisticated than others. Some of them are primary evaluation tools, others just support evaluation, next to several other features. But none of the tools described above is designed to be embedded in other software systems as a library. They usually only provide a user interface but not an \gls{API}. But most importantly, every tool only supports the evaluation of extraction results, not the runtime performance of different extractors.

Because none of these systems meets the requirements a new framework had to be designed which is based on the concepts of embeddability, extensibility, modularity and which combines the execution and evalutation of information extraction systems.